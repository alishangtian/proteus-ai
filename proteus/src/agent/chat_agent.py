"""Chat Agent æ¨¡å— - å¤„ç†çº¯èŠå¤©æ¨¡å¼çš„å¯¹è¯"""

import logging
import json
import time
import os
from string import Template
from datetime import datetime
from typing import Optional, List, Dict, Any
from src.api.stream_manager import StreamManager
from src.api.llm_api import call_llm_api_stream, call_llm_api_with_tools_stream
from src.utils.tool_converter import load_tools_from_yaml
from src.utils.langfuse_wrapper import langfuse_wrapper
from src.agent.prompt.chat_agent_system_prompt import CHAT_AGENT_SYSTEM_PROMPT
from src.agent.prompt.chat_agent_system_prompt_v2 import CHAT_AGENT_SYSTEM_PROMPT_V2
from src.utils.redis_cache import get_redis_connection
from src.manager.tool_memory_manager import ToolMemoryManager
from src.manager.conversation_manager import conversation_manager
import uuid
from src.api.events import (
    create_agent_start_event,
    create_agent_complete_event,
    create_agent_stream_thinking_event,
    create_complete_event,
    create_error_event,
    create_retry_event,
    create_usage_event,
)

logger = logging.getLogger(__name__)


class ChatAgent:
    """Chat Agent ç±» - å¤„ç†çº¯èŠå¤©æ¨¡å¼çš„å¯¹è¯

    è¯¥ç±»å°è£…äº† chat æ¨¡å¼ä¸‹çš„æ‰€æœ‰é€»è¾‘ï¼ŒåŒ…æ‹¬ï¼š
    - æµå¼ LLM è°ƒç”¨
    - å·¥å…·è°ƒç”¨æ”¯æŒ
    - Thinking å†…å®¹å¤„ç†
    - å¯¹è¯å†å²ç®¡ç†
    """

    def __init__(
        self,
        stream_manager: StreamManager,
        model_name: Optional[str] = None,
        enable_tools: bool = False,
        tool_choices: Optional[List[str]] = None,
        max_tool_iterations: int = 5,
        conversation_id: Optional[str] = None,
        conversation_round: int = 5,
        enable_tool_memory: bool = True,
        enable_skills_memory: bool = True,
        user_name: Optional[str] = None,
        system_prompt: Optional[str] = None,
    ):
        """åˆå§‹åŒ– ChatAgent

        Args:
            stream_manager: æµç®¡ç†å™¨å®ä¾‹
            model_name: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º "deepseek-chat"
            enable_tools: æ˜¯å¦å¯ç”¨å·¥å…·è°ƒç”¨
            tool_choices: æŒ‡å®šçš„å·¥å…·åˆ—è¡¨
            max_tool_iterations: æœ€å¤§å·¥å…·è°ƒç”¨è¿­ä»£æ¬¡æ•°
            enable_tool_memory: æ˜¯å¦å¯ç”¨å·¥å…·è®°å¿†åŠŸèƒ½
        """
        self.stream_manager = stream_manager
        self.model_name = model_name or "deepseek-chat"
        self.enable_tools = enable_tools
        self.tool_choices = tool_choices
        self.max_tool_iterations = max_tool_iterations
        self.conversation_id = conversation_id
        self.conversation_round = conversation_round
        self.enable_tool_memory = enable_tool_memory
        self.enable_skills_memory = enable_skills_memory

        # åˆå§‹åŒ–å·¥å…·è®°å¿†ç®¡ç†å™¨
        self.tool_memory_manager = ToolMemoryManager()

        # åˆå§‹åŒ–skillsè®°å¿†ç®¡ç†å™¨
        self.user_name = user_name
        self.system_prompt = system_prompt or CHAT_AGENT_SYSTEM_PROMPT_V2

    @langfuse_wrapper.dynamic_observe(name="chat_agent_run")
    async def run(
        self,
        chat_id: str,
        text: str,
        file_analysis_context: str = "",
    ) -> str:
        """è¿è¡Œ Chat Agent

        Args:
            chat_id: èŠå¤©ä¼šè¯ID
            text: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
            file_analysis_context: æ–‡ä»¶åˆ†æä¸Šä¸‹æ–‡
            conversation_id: ä¼šè¯IDï¼ˆç”¨äºä¿å­˜å†å²ï¼‰

        Returns:
            str: æœ€ç»ˆå“åº”æ–‡æœ¬
        """
        logger.info(
            f"[{chat_id}] å¼€å§‹ chat æ¨¡å¼è¯·æ±‚ï¼ˆæµå¼ï¼‰ï¼Œå·¥å…·è°ƒç”¨: {self.enable_tools}"
        )

        try:
            # å‘é€ agent_start äº‹ä»¶
            if self.stream_manager:
                event = await create_agent_start_event(text)
                await self.stream_manager.send_message(chat_id, event)

            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = []

            # 1. å…ˆåŠ è½½å†å²ä¼šè¯
            if self.conversation_id:
                conversation_history = conversation_manager.load_conversation_history(
                    self.conversation_id, max_messages=self.conversation_round * 3
                )
                # conversation_history = self._load_conversation_history()
                if conversation_history:
                    messages.extend(conversation_history)

            # 2. æ£€ç´¢ç›¸å…³skillsè®°å¿†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            skills_memories = []
            if self.enable_skills_memory:
                skills_memories = await self._retrieve_skills_memories(chat_id, text)
            file_added = False
            # å¦‚æœmessagesä¸ºç©ºå°±æ·»åŠ åˆ°ç¬¬ä¸€æ¡è§’è‰²ä¸º system
            all_values = {"CURRENT_TIME": datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

            # æ„å»ºskillsè®°å¿†å†…å®¹
            skills_memories_content = self._build_skills_memories_content(
                skills_memories
            )
            all_values["SKILLS_MEMORIES"] = skills_memories_content
            all_values["LANGUAGE"] = os.getenv("LANGUAGE", "ä¸­æ–‡")

            if not messages:
                system_message = {
                    "role": "system",
                    "content": CHAT_AGENT_SYSTEM_PROMPT_V2,
                }
                if file_analysis_context:
                    system_message["content"] = (
                        f"{system_message['content']} {file_analysis_context}"
                    )
                    file_added = True
                # ç«‹å³ä¿å­˜ system æ¶ˆæ¯åˆ° Redis
                if self.conversation_id:
                    conversation_manager.save_message(
                        conversation_id=self.conversation_id, message=system_message
                    )
                    # self._save_conversation_to_redis(message=system_message)
                    logger.info(f"[{chat_id}] å·²ä¿å­˜æ–‡ä»¶ä¸Šä¸‹æ–‡æ¶ˆæ¯åˆ° Redis")
                system_message["content"] = Template(
                    system_message["content"]
                ).safe_substitute(all_values)
                messages.append(system_message)
            else:
                # åŒ…å«å˜é‡éœ€è¦æ›¿æ¢çš„ system content éœ€è¦è¿›è¡Œæ›¿æ¢
                system_message = messages[0]
                system_message["content"] = Template(
                    system_message["content"]
                ).safe_substitute(all_values)

            user_message = {
                "role": "user",
                "content": text,
            }

            # 2. æ·»åŠ æ–‡ä»¶ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰
            if not file_added and file_analysis_context:
                user_message = {
                    "role": "user",
                    "content": f" {file_analysis_context}\n\nè¯·æ ¹æ®æ–‡ä»¶å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼š{text}",
                }

            # 3. æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
            messages.append(user_message)

            # ç«‹å³ä¿å­˜ç”¨æˆ·æ¶ˆæ¯åˆ° Redisï¼ˆä¿å­˜å®Œæ•´çš„ message å¯¹è±¡ï¼‰
            if self.conversation_id:
                conversation_manager.save_message(
                    conversation_id=self.conversation_id, message=user_message
                )
                # self._save_conversation_to_redis(message=user_message)
                logger.info(f"[{chat_id}] å·²ä¿å­˜ç”¨æˆ·æ¶ˆæ¯åˆ° Redis")

            # åŠ è½½å·¥å…·ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            tools = None
            tool_map = {}
            if self.enable_tools:
                tools, tool_map = await self._load_tools_with_tracking(chat_id)

            # chat æ¨¡å¼ä¸‹ï¼Œå§‹ç»ˆä¼ é€’ enable_thinking=Trueï¼Œè®© API å±‚æ ¹æ®å“åº”å†³å®š
            enable_thinking = True

            logger.info(
                f"[{chat_id}] chat æ¨¡å¼ä½¿ç”¨æ¨¡å‹: {self.model_name}ï¼Œå°†æ ¹æ® API å“åº”è‡ªåŠ¨å¤„ç† thinking å†…å®¹"
            )

            # å·¥å…·è°ƒç”¨å¾ªç¯
            max_iterations = (
                self.max_tool_iterations if self.enable_tools and tools else 1
            )
            tool_iteration = 0
            while tool_iteration < max_iterations:
                # æ‰§è¡Œä¸€æ¬¡ LLM ç”Ÿæˆè¿­ä»£
                (
                    response_text,
                    thinking_text,
                    tool_calls,
                    accumulated_usage,
                    thinking_type,
                    reasoning_details,
                ) = await self._execute_llm_generation(
                    chat_id=chat_id,
                    messages=self._filter_messages(
                        messages
                    ),  # è¿‡æ»¤æ¶ˆæ¯ï¼Œåªä¿ç•™æœ€æ–°çš„æ€è€ƒæ¶ˆæ¯
                    tools=tools,
                    enable_thinking=enable_thinking,
                    tool_iteration=tool_iteration,
                )

                # ä¿å­˜æœ€ç»ˆå“åº”
                final_response_text = response_text

                # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¯´æ˜æ¨¡å‹è¿”å›äº†æœ€ç»ˆç­”æ¡ˆï¼Œé€€å‡ºå¾ªç¯
                if not tool_calls:
                    logger.info(f"[{chat_id}] æ¨¡å‹è¿”å›æœ€ç»ˆç­”æ¡ˆï¼Œç»“æŸå·¥å…·è°ƒç”¨å¾ªç¯")
                    if thinking_text:
                        thought_message = {
                            "role": "assistant",
                            thinking_type: thinking_text,
                            "content": response_text,
                        }
                        messages.append(thought_message)
                    break

                # æ‰§è¡Œå·¥å…·è°ƒç”¨
                tool_messages = await self._execute_tools(
                    chat_id=chat_id,
                    tool_calls=tool_calls,
                    tool_iteration=tool_iteration,
                )

                # å°†åŠ©æ‰‹æ¶ˆæ¯ï¼ˆåŒ…å«å·¥å…·è°ƒç”¨ï¼‰æ·»åŠ åˆ°messages
                assistant_message = {
                    "role": "assistant",
                    thinking_type: thinking_text,
                    "content": response_text,
                    "tool_calls": tool_calls,
                    "reasoning_details": reasoning_details,
                }
                messages.append(assistant_message)

                # ç«‹å³ä¿å­˜åŠ©æ‰‹æ¶ˆæ¯åˆ° Redisï¼ˆä¿å­˜å®Œæ•´çš„ message å¯¹è±¡ï¼‰
                if self.conversation_id:
                    conversation_manager.save_message(
                        conversation_id=self.conversation_id, message=assistant_message
                    )
                    # self._save_conversation_to_redis(message=assistant_message)
                    logger.info(
                        f"[{chat_id}] å·²ä¿å­˜åŠ©æ‰‹æ¶ˆæ¯ï¼ˆåŒ…å« {len(tool_calls)} ä¸ªå·¥å…·è°ƒç”¨ï¼‰åˆ° Redis"
                    )

                # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°messagesï¼Œå¹¶ç«‹å³ä¿å­˜åˆ° Redis
                for tool_msg in tool_messages:
                    messages.append(tool_msg)
                    # ç«‹å³ä¿å­˜æ¯ä¸ªå·¥å…·è°ƒç”¨ç»“æœåˆ° Redisï¼ˆä¿å­˜å®Œæ•´çš„ message å¯¹è±¡ï¼‰
                    if self.conversation_id:
                        conversation_manager.save_message(
                            conversation_id=self.conversation_id, message=tool_msg
                        )
                        # self._save_conversation_to_redis(message=tool_msg)

                if self.conversation_id and tool_messages:
                    logger.info(
                        f"[{chat_id}] å·²ä¿å­˜ {len(tool_messages)} ä¸ªå·¥å…·è°ƒç”¨ç»“æœåˆ° Redis"
                    )

                # å¢åŠ è¿­ä»£è®¡æ•°
                tool_iteration += 1
                logger.info(
                    f"[{chat_id}] å®Œæˆç¬¬ {tool_iteration} æ¬¡å·¥å…·è°ƒç”¨ï¼Œç»§ç»­ä¸‹ä¸€è½®æ¨ç†"
                )

            # å‘é€å®Œæˆäº‹ä»¶
            if self.stream_manager:
                await self.stream_manager.send_message(
                    chat_id, await create_complete_event()
                )

            # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œä¿å­˜æœ€ç»ˆçš„åŠ©æ‰‹å›ç­”åˆ° Redis
            # ï¼ˆå¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼ŒåŠ©æ‰‹æ¶ˆæ¯å·²ç»åœ¨å·¥å…·è°ƒç”¨å¾ªç¯ä¸­ä¿å­˜äº†ï¼‰
            if self.conversation_id:
                # æ„å»ºæœ€ç»ˆåŠ©æ‰‹æ¶ˆæ¯å¯¹è±¡
                final_assistant_message = {
                    "role": "assistant",
                    "content": final_response_text,
                }
                conversation_manager.save_message(
                    self.conversation_id, final_assistant_message
                )
                # self._save_conversation_to_redis(message=final_assistant_message)
                logger.info(f"[{chat_id}] å·²ä¿å­˜æœ€ç»ˆåŠ©æ‰‹å›ç­”åˆ° Redis")

            logger.info(f"[{chat_id}] chat æ¨¡å¼è¯·æ±‚å®Œæˆï¼ˆæµå¼ï¼‰")

            # å¼‚æ­¥å¤„ç†skillsè®°å¿†ç”Ÿæˆï¼ˆå¦‚æœå¯ç”¨ä¸”æˆåŠŸè§£å†³é—®é¢˜ï¼‰
            if self.enable_skills_memory and final_response_text:
                await self._async_process_skills_memory(
                    chat_id=chat_id,
                    user_query=text,
                    final_result=final_response_text,
                    tool_calls_history=self._get_tool_calls_from_messages(messages),
                    is_success=True,
                )

            return final_response_text

        except Exception as e:
            error_msg = f"chat æ¨¡å¼å¤„ç†å¤±è´¥: {str(e)}"
            logger.error(f"[{chat_id}] {error_msg}", exc_info=True)

            if self.stream_manager:
                await self.stream_manager.send_message(
                    chat_id, await create_error_event(error_msg)
                )
            raise

    @langfuse_wrapper.dynamic_observe()
    async def _load_tools_with_tracking(
        self, chat_id: str
    ) -> tuple[Optional[List[Dict]], Dict[str, Dict]]:
        """åŠ è½½å·¥å…·ï¼ˆå¸¦è¿½è¸ªï¼‰

        Args:
            chat_id: èŠå¤©ä¼šè¯ID

        Returns:
            tuple: (å·¥å…·åˆ—è¡¨, å·¥å…·æ˜ å°„å­—å…¸)
        """
        tools = None
        tool_map = {}

        try:
            if self.tool_choices:
                # ä½¿ç”¨æŒ‡å®šçš„å·¥å…·
                tools = load_tools_from_yaml(node_names=self.tool_choices)
                logger.info(f"[{chat_id}] åŠ è½½æŒ‡å®šå·¥å…·: {self.tool_choices}")
            else:
                # åŠ è½½æ‰€æœ‰å·¥å…·
                tools = load_tools_from_yaml()
                logger.info(f"[{chat_id}] åŠ è½½æ‰€æœ‰å¯ç”¨å·¥å…·")

            # å¦‚æœå¯ç”¨äº†å·¥å…·è®°å¿†ï¼Œä¸ºæ¯ä¸ªå·¥å…·é™„åŠ è®°å¿†ä¿¡æ¯
            if self.enable_tool_memory and tools:
                for tool in tools:
                    tool_name = tool["function"]["name"]
                    # å¼‚æ­¥åŠ è½½å·¥å…·è®°å¿†
                    tool_memory = await self.tool_memory_manager.load_tool_memory(
                        tool_name=tool_name, user_name=self.user_name
                    )
                    if tool_memory:
                        # å°†å·¥å…·è®°å¿†é™„åŠ åˆ°å·¥å…·æè¿°ä¸­
                        original_description = tool["function"]["description"]
                        enhanced_description = (
                            f"{original_description}\n\n"
                            f"ã€å·¥å…·ä½¿ç”¨ç»éªŒã€‘{tool_memory}"
                        )
                        tool["function"]["description"] = enhanced_description
                        logger.info(f"[{chat_id}] ä¸ºå·¥å…· '{tool_name}' é™„åŠ äº†ä½¿ç”¨ç»éªŒ")

            # æ„å»ºå·¥å…·æ˜ å°„
            for tool in tools:
                tool_map[tool["function"]["name"]] = tool

            logger.info(f"[{chat_id}] æˆåŠŸåŠ è½½ {len(tools)} ä¸ªå·¥å…·")
        except Exception as e:
            logger.error(f"[{chat_id}] åŠ è½½å·¥å…·å¤±è´¥: {str(e)}")
            tools = None
            raise

        return tools, tool_map

    @langfuse_wrapper.dynamic_observe()
    async def _execute_llm_generation(
        self,
        chat_id: str,
        messages: List[Dict[str, Any]],
        tools: Optional[List[Dict]],
        enable_thinking: bool,
        tool_iteration: int,
    ) -> tuple[str, str, Optional[List], Dict]:
        """æ‰§è¡Œ LLM ç”Ÿæˆï¼ˆå¸¦è¿½è¸ªï¼‰

        Args:
            chat_id: èŠå¤©ä¼šè¯ID
            messages: æ¶ˆæ¯åˆ—è¡¨
            tools: å·¥å…·åˆ—è¡¨
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
            tool_iteration: å½“å‰è¿­ä»£æ¬¡æ•°

        Returns:
            tuple: (å“åº”æ–‡æœ¬, æ€è€ƒæ–‡æœ¬, å·¥å…·è°ƒç”¨åˆ—è¡¨, ä½¿ç”¨æƒ…å†µ)
        """
        response_text = ""
        thinking_text = ""
        thinking_type = ""
        reasoning_details = []
        first_content_chunk_sent = False
        has_thinking_content = False
        tool_calls = None
        accumulated_usage = {}
        start_time = time.time()

        try:
            langfuse_instance = langfuse_wrapper.get_langfuse_instance()
            with langfuse_instance.start_as_current_span(name="llm-call") as span:
                # åˆ›å»ºåµŒå¥—çš„generation span
                span.update_trace(session_id=chat_id)
                with span.start_as_current_generation(
                    name="generate-response",
                    model=self.model_name,
                    input={"prompt": messages},
                    model_parameters={
                        "temperature": 0.7,
                        "enable_thinking": enable_thinking,
                    },
                    metadata={
                        "chat_id": chat_id,
                        "tool_iteration": tool_iteration,
                        "has_tools": tools is not None,
                    },
                ) as generation:
                    # æ ¹æ®æ˜¯å¦æœ‰å·¥å…·é€‰æ‹©ä¸åŒçš„API
                    if tools:
                        # ä½¿ç”¨æ”¯æŒå·¥å…·è°ƒç”¨çš„æµå¼ API
                        async for chunk in call_llm_api_with_tools_stream(
                            messages=messages,
                            tools=tools,
                            model_name=self.model_name,
                            request_id=chat_id,
                            enable_thinking=enable_thinking,
                        ):
                            chunk_type = chunk.get("type")

                            if chunk_type == "thinking":
                                has_thinking_content = True
                                thinking_content = chunk.get("content", "")
                                thinking_text += thinking_content
                                thinking_type = chunk.get("thinking_type")
                                if self.stream_manager and thinking_content:
                                    event = await create_agent_stream_thinking_event(
                                        thinking_content
                                    )
                                    await self.stream_manager.send_message(
                                        chat_id, event
                                    )

                            if chunk_type == "reasoning_details":
                                reasoning_details = chunk.get("content", [])

                            if chunk_type == "content":
                                if (
                                    not first_content_chunk_sent
                                    and has_thinking_content
                                ):
                                    first_content_chunk_sent = True
                                    if self.stream_manager:
                                        event = (
                                            await create_agent_stream_thinking_event(
                                                "[THINKING_DONE]"
                                            )
                                        )
                                        await self.stream_manager.send_message(
                                            chat_id, event
                                        )
                                content = chunk.get("content", "")
                                response_text += content
                                if self.stream_manager and content:
                                    event = await create_agent_complete_event(content)
                                    await self.stream_manager.send_message(
                                        chat_id, event
                                    )

                            if chunk_type == "tool_calls":
                                if (
                                    not first_content_chunk_sent
                                    and has_thinking_content
                                ):
                                    first_content_chunk_sent = True
                                    if self.stream_manager:
                                        event = (
                                            await create_agent_stream_thinking_event(
                                                "[THINKING_DONE]"
                                            )
                                        )
                                        await self.stream_manager.send_message(
                                            chat_id, event
                                        )
                                tool_calls = chunk.get("tool_calls", [])
                                logger.info(
                                    f"[{chat_id}] æ¨¡å‹è¯·æ±‚è°ƒç”¨ {len(tool_calls)} ä¸ªå·¥å…·"
                                )
                                for tool_call in tool_calls:
                                    if tool_call.get("id") is None:
                                        tool_call["id"] = "call_" + str(uuid.uuid4())

                            if chunk_type == "usage":
                                if (
                                    not first_content_chunk_sent
                                    and has_thinking_content
                                ):
                                    first_content_chunk_sent = True
                                    if self.stream_manager:
                                        event = (
                                            await create_agent_stream_thinking_event(
                                                "[THINKING_DONE]"
                                            )
                                        )
                                        await self.stream_manager.send_message(
                                            chat_id, event
                                        )
                                accumulated_usage = chunk.get("usage", {})
                                logger.info(
                                    f"[{chat_id}] Token ä½¿ç”¨æƒ…å†µ: {accumulated_usage}"
                                )
                                if self.stream_manager and accumulated_usage:
                                    event = await create_usage_event(accumulated_usage)
                                    await self.stream_manager.send_message(
                                        chat_id, event
                                    )

                            if chunk_type == "retry":
                                retry_msg = chunk.get("error", "æœªçŸ¥é”™è¯¯")
                                logger.error(f"[{chat_id}] æµå¼è°ƒç”¨é”™è¯¯: {retry_msg}")
                                if self.stream_manager:
                                    await self.stream_manager.send_message(
                                        chat_id, await create_retry_event(retry_msg)
                                    )

                            if chunk_type == "error":
                                if (
                                    not first_content_chunk_sent
                                    and has_thinking_content
                                ):
                                    first_content_chunk_sent = True
                                    if self.stream_manager:
                                        event = (
                                            await create_agent_stream_thinking_event(
                                                "[THINKING_DONE]"
                                            )
                                        )
                                        await self.stream_manager.send_message(
                                            chat_id, event
                                        )
                                error_msg = chunk.get("error", "æœªçŸ¥é”™è¯¯")
                                logger.error(f"[{chat_id}] æµå¼è°ƒç”¨é”™è¯¯: {error_msg}")
                                if self.stream_manager:
                                    await self.stream_manager.send_message(
                                        chat_id, await create_error_event(error_msg)
                                    )
                                raise Exception(error_msg)
                    else:
                        # ä½¿ç”¨æ™®é€šçš„æµå¼ APIï¼ˆä¸æ”¯æŒå·¥å…·è°ƒç”¨ï¼‰
                        async for chunk in call_llm_api_stream(
                            messages=messages,
                            model_name=self.model_name,
                            request_id=chat_id,
                            enable_thinking=enable_thinking,
                        ):
                            chunk_type = chunk.get("type")

                            if chunk_type == "thinking":
                                has_thinking_content = True
                                thinking_content = chunk.get("content", "")
                                thinking_text += thinking_content
                                thinking_type = chunk.get("thinking_type")
                                if self.stream_manager and thinking_content:
                                    event = await create_agent_stream_thinking_event(
                                        thinking_content
                                    )
                                    await self.stream_manager.send_message(
                                        chat_id, event
                                    )

                            if chunk_type == "content":
                                if (
                                    not first_content_chunk_sent
                                    and has_thinking_content
                                ):
                                    first_content_chunk_sent = True
                                    if self.stream_manager:
                                        event = (
                                            await create_agent_stream_thinking_event(
                                                "[THINKING_DONE]"
                                            )
                                        )
                                        await self.stream_manager.send_message(
                                            chat_id, event
                                        )
                                content = chunk.get("content", "")
                                response_text += content
                                if self.stream_manager and content:
                                    event = await create_agent_complete_event(content)
                                    await self.stream_manager.send_message(
                                        chat_id, event
                                    )

                            if chunk_type == "usage":
                                accumulated_usage = chunk.get("usage", {})
                                logger.info(
                                    f"[{chat_id}] Token ä½¿ç”¨æƒ…å†µ: {accumulated_usage}"
                                )
                                if self.stream_manager and accumulated_usage:
                                    event = await create_usage_event(accumulated_usage)
                                    await self.stream_manager.send_message(
                                        chat_id, event
                                    )

                            if chunk_type == "retry":
                                retry_msg = chunk.get("error", "æœªçŸ¥é”™è¯¯")
                                logger.error(f"[{chat_id}] æµå¼è°ƒç”¨é”™è¯¯: {retry_msg}")
                                if self.stream_manager:
                                    await self.stream_manager.send_message(
                                        chat_id, await create_retry_event(retry_msg)
                                    )

                            if chunk_type == "error":
                                error_msg = chunk.get("error", "æœªçŸ¥é”™è¯¯")
                                logger.error(f"[{chat_id}] æµå¼è°ƒç”¨é”™è¯¯: {error_msg}")
                                if self.stream_manager:
                                    await self.stream_manager.send_message(
                                        chat_id, await create_error_event(error_msg)
                                    )
                                raise Exception(error_msg)

                    # è®¡ç®—æ‰§è¡Œæ—¶é—´
                    execution_time = time.time() - start_time

                    # æ„å»ºè¾“å‡ºå†…å®¹
                    output_content = {
                        "response": response_text,
                        "thinking": thinking_text if thinking_text else None,
                        "tool_calls": tool_calls if tool_calls else None,
                    }

                    # å°è¯•ä½¿ç”¨çœŸå®usageå­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ä¼°ç®—
                    usage_details = {
                        "input_usage": accumulated_usage.get("prompt_tokens", 0),
                        "output_usage": accumulated_usage.get("completion_tokens", 0),
                    }

                    # æ›´æ–° generation
                    generation.update(
                        output=output_content,
                        usage_details=usage_details,
                        # cost_details={
                        #     "total_cost": accumulated_usage.get("total_cost", 0.0)
                        # },
                        metadata={"execution_time": execution_time},
                    )

                    # è¯„åˆ† - æ ¹æ®æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨å’Œå“åº”è´¨é‡è¯„åˆ†
                    relevance_score = 0.95 if response_text else 0.5
                    generation.score(
                        name="relevance", value=relevance_score, data_type="NUMERIC"
                    )

                    logger.info(
                        f"[{chat_id}] LLMç”Ÿæˆå®Œæˆ (iteration {tool_iteration}, "
                        f"è€—æ—¶: {execution_time:.2f}s, tokens: {usage_details['input_usage']+usage_details['output_usage']})"
                    )

            return (
                response_text,
                thinking_text,
                tool_calls,
                accumulated_usage,
                thinking_type,
                reasoning_details,
            )

        except Exception as e:
            execution_time = time.time() - start_time if "start_time" in locals() else 0

            # å°è¯•æ›´æ–° generation span çš„é”™è¯¯çŠ¶æ€
            if "generation" in locals():
                try:
                    if hasattr(generation, "update"):
                        generation.update(
                            output={"error": str(e)},
                            status_message=f"LLM call failed: {str(e)}",
                            metadata={"execution_time": execution_time},
                        )
                except Exception as update_error:
                    logger.warning(f"Failed to update generation span: {update_error}")

            logger.error(f"[{chat_id}] LLMç”Ÿæˆå¤±è´¥: {str(e)}", exc_info=True)
            raise

    @langfuse_wrapper.dynamic_observe()
    async def _execute_tools(
        self,
        chat_id: str,
        tool_calls: List[Dict],
        tool_iteration: int,
    ) -> List[Dict[str, Any]]:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼ˆå¸¦è¿½è¸ªï¼‰

        Args:
            chat_id: èŠå¤©ä¼šè¯ID
            tool_calls: å·¥å…·è°ƒç”¨åˆ—è¡¨
            tool_iteration: å½“å‰è¿­ä»£æ¬¡æ•°

        Returns:
            List[Dict[str, Any]]: å·¥å…·æ‰§è¡Œç»“æœæ¶ˆæ¯åˆ—è¡¨
        """
        from src.api.tool_executor import ToolExecutor

        tool_executor = ToolExecutor(
            stream_manager=self.stream_manager,
            max_retries=3,
            retry_delay=1.0,
        )

        # æ‰¹é‡æ‰§è¡Œå·¥å…·è°ƒç”¨å¹¶æ”¶é›†ç»“æœ
        start_time = time.time()
        tool_messages = await tool_executor.execute_tool_calls(
            tool_calls=tool_calls, chat_id=chat_id
        )
        execution_time = time.time() - start_time

        logger.info(
            f"[{chat_id}] å·¥å…·æ‰§è¡Œå®Œæˆ (iteration {tool_iteration}, è€—æ—¶: {execution_time:.2f}s)"
        )

        # å¼‚æ­¥å¤„ç†å·¥å…·è®°å¿†æ›´æ–°ï¼ˆä¸é˜»å¡ä¸»æµç¨‹ï¼‰
        if self.enable_tool_memory:
            await self._async_process_tool_memory(
                chat_id=chat_id,
                tool_calls=tool_calls,
                tool_messages=tool_messages,
                tool_iteration=tool_iteration,
                user_name=self.user_name,
            )

        return tool_messages

    @langfuse_wrapper.dynamic_observe()
    async def _async_process_tool_memory(
        self,
        chat_id: str,
        tool_calls: List[Dict],
        tool_messages: List[Dict[str, Any]],
        tool_iteration: int,
        user_name: str,
    ) -> None:
        """å¼‚æ­¥å¤„ç†å·¥å…·è®°å¿†æ›´æ–°

        Args:
            chat_id: èŠå¤©ä¼šè¯ID
            tool_calls: å·¥å…·è°ƒç”¨åˆ—è¡¨
            tool_messages: å·¥å…·æ‰§è¡Œç»“æœæ¶ˆæ¯åˆ—è¡¨
            tool_iteration: å½“å‰è¿­ä»£æ¬¡æ•°
        """
        try:
            # æ„å»ºå·¥å…·è°ƒç”¨å’Œç»“æœçš„æ˜ å°„
            tool_results = {}
            for tool_msg in tool_messages:
                if tool_msg.get("role") == "tool":
                    tool_call_id = tool_msg.get("tool_call_id")
                    tool_results[tool_call_id] = {
                        "content": tool_msg.get("content", ""),
                        "name": tool_msg.get("name", ""),
                    }

            # ä¸ºæ¯ä¸ªå·¥å…·è°ƒç”¨å¼‚æ­¥å¤„ç†è®°å¿†
            for tool_call in tool_calls:
                tool_call_id = tool_call.get("id")
                tool_name = tool_call.get("function", {}).get("name")
                action_input = tool_call.get("function", {}).get("arguments", {})

                # è§£æå‚æ•°
                try:
                    if isinstance(action_input, str):
                        action_input = json.loads(action_input)
                except (json.JSONDecodeError, TypeError):
                    action_input = {}

                # è·å–å·¥å…·æ‰§è¡Œç»“æœ
                tool_result = tool_results.get(tool_call_id, {})
                observation = tool_result.get("content", "")

                # å¼‚æ­¥è°ƒç”¨å·¥å…·è®°å¿†å¤„ç†ï¼ˆä¸é˜»å¡ä¸»æµç¨‹ï¼‰
                import asyncio

                asyncio.create_task(
                    self._process_single_tool_memory(
                        chat_id=chat_id,
                        tool_name=tool_name,
                        action_input=action_input,
                        observation=observation,
                        tool_iteration=tool_iteration,
                    )
                )

            logger.info(f"[{chat_id}] å·²å¯åŠ¨ {len(tool_calls)} ä¸ªå·¥å…·çš„è®°å¿†å¼‚æ­¥å¤„ç†")

        except Exception as e:
            logger.error(f"[{chat_id}] å¼‚æ­¥å¤„ç†å·¥å…·è®°å¿†å¤±è´¥: {str(e)}", exc_info=True)

    @langfuse_wrapper.dynamic_observe()
    async def _process_single_tool_memory(
        self,
        chat_id: str,
        tool_name: str,
        action_input: Dict[str, Any],
        observation: str,
        is_error: bool = False,
        error_message: str = None,
        tool_iteration: int = 25,
    ) -> str:
        """å¤„ç†å•ä¸ªå·¥å…·çš„è®°å¿†æ›´æ–°

        Args:
            chat_id: èŠå¤©ä¼šè¯ID
            tool_name: å·¥å…·åç§°
            action_input: å·¥å…·è¾“å…¥å‚æ•°
            observation: å·¥å…·æ‰§è¡Œç»“æœ
            is_error: æ˜¯å¦ä¸ºé”™è¯¯æ‰§è¡Œ
            error_message: é”™è¯¯ä¿¡æ¯
            tool_iteration: å½“å‰è¿­ä»£æ¬¡æ•°
        """
        try:
            # è·å–å½“å‰ç”¨æˆ·è¾“å…¥ï¼ˆä»å¯¹è¯å†å²ä¸­è·å–ï¼‰
            user_query = await self._get_current_user_query()

            # è°ƒç”¨å·¥å…·è®°å¿†ç®¡ç†å™¨å¤„ç†è®°å¿†
            result = await self.tool_memory_manager.process_tool_memory(
                tool_name=tool_name,
                action_input=action_input,
                observation=observation,
                chat_id=chat_id,
                is_error=is_error,
                error_message=error_message,
                user_query=user_query,
                model_name=self.model_name,
                conversation_id=self.conversation_id,
                user_name=self.user_name,
            )

            logger.info(
                f"[{chat_id}] å·¥å…· '{tool_name}' çš„è®°å¿†å¤„ç†å®Œæˆ (iteration {tool_iteration})"
            )

            return result

        except Exception as e:
            logger.error(
                f"[{chat_id}] å¤„ç†å·¥å…· '{tool_name}' è®°å¿†å¤±è´¥: {str(e)}", exc_info=True
            )

    def _filter_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        è¿‡æ»¤ messages åˆ—è¡¨ï¼Œåªä¿ç•™æœ€æ–°çš„ä¸€ä¸ªåŒ…å« 'reasoning' æˆ– 'reasoning_content' å­—æ®µçš„æ¶ˆæ¯ã€‚
        å¦‚æœåŸå§‹ messages åˆ—è¡¨ä¸åŒ…å«è¿™äº›å­—æ®µçš„æ¶ˆæ¯ï¼Œåˆ™è¿”å›åŸå§‹åˆ—è¡¨ã€‚
        """
        # åˆ›å»ºå¯ä¿®æ”¹çš„å‰¯æœ¬
        processed_messages = messages[:]

        # æ‰¾åˆ°æ‰€æœ‰åŒ…å« 'reasoning' æˆ– 'reasoning_content' å­—æ®µï¼Œä¸”ä¸åŒ…å« 'tool_calls' å­—æ®µçš„æ¶ˆæ¯çš„ç´¢å¼•
        removable_thinking_message_indices = []
        for i, message in enumerate(processed_messages):
            is_thinking_message = isinstance(message, dict) and any(
                key in message for key in ["reasoning", "reasoning_content"]
            )
            has_tool_calls = (
                isinstance(message, dict)
                and "tool_calls" in message
                and message["tool_calls"] is not None
            )

            if is_thinking_message and not has_tool_calls:
                removable_thinking_message_indices.append(i)

        # å¦‚æœå¯ç§»é™¤çš„æ€è€ƒæ¶ˆæ¯çš„æ•°é‡å¤§äº1ï¼Œåˆ™ç§»é™¤æœ€é å‰çš„é‚£ä¸ª
        if len(removable_thinking_message_indices) > 1:
            first_removable_thinking_message_index = removable_thinking_message_indices[
                0
            ]
            del processed_messages[first_removable_thinking_message_index]
            logger.info(
                f"ç§»é™¤äº†æœ€é å‰çš„æ€è€ƒæ¶ˆæ¯ï¼ˆä¸å«tool_callsï¼‰ï¼ŒåŸç´¢å¼•: {first_removable_thinking_message_index}"
            )

        return processed_messages

    async def _get_current_user_query(self) -> Optional[str]:
        """è·å–å½“å‰ç”¨æˆ·æŸ¥è¯¢

        Returns:
            Optional[str]: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
        """
        try:
            if not self.conversation_id:
                return None

            # ä»RedisåŠ è½½æœ€è¿‘çš„å¯¹è¯å†å²
            conversation_history = conversation_manager.load_conversation_history(
                self.conversation_id, max_messages=self.conversation_round * 3
            )
            # conversation_history = self._load_conversation_history()
            if not conversation_history:
                return None

            # æŸ¥æ‰¾æœ€è¿‘çš„ç”¨æˆ·æ¶ˆæ¯
            for message in reversed(conversation_history):
                if message.get("role") == "user":
                    return message.get("content", "")

            return None
        except Exception as e:
            logger.warning(f"è·å–å½“å‰ç”¨æˆ·æŸ¥è¯¢å¤±è´¥: {str(e)}")
            return None

    @langfuse_wrapper.dynamic_observe()
    def _save_conversation_to_redis(
        self,
        message: Dict[str, Any],
        expire_hours: int = 12,
        max_retries: int = 3,
        retry_delay: float = 0.1,
    ):
        """å°†å®Œæ•´çš„ message å¯¹è±¡ä¿å­˜åˆ° Redis list ä¸­ï¼Œä¿æŒä¸è¿è¡Œæ—¶ messages ç»“æ„ä¸¥æ ¼ä¸€è‡´

        Args:
            message: å®Œæ•´çš„æ¶ˆæ¯å¯¹è±¡ï¼Œæ ¼å¼ä¸ messages åˆ—è¡¨ä¸­çš„å…ƒç´ å®Œå…¨ä¸€è‡´
                    ä¾‹å¦‚: {"role": "user", "content": "..."}
                         {"role": "assistant", "content": "...", "tool_calls": [...]}
                         {"role": "tool", "content": "...", "tool_call_id": "...", "name": "..."}
            expire_hours: è¿‡æœŸæ—¶é—´ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤12å°æ—¶
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤3æ¬¡
            retry_delay: é‡è¯•å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤0.1ç§’
        """
        retry_count = 0
        last_exception = None

        while retry_count <= max_retries:
            try:
                redis_cache = get_redis_connection()
                redis_key = f"conversation:{self.conversation_id}"
                current_timestamp = time.time()

                # æ„é€ è®°å½•ï¼šä¿å­˜å®Œæ•´çš„ message å¯¹è±¡ + æ—¶é—´æˆ³
                record = {
                    "timestamp": current_timestamp,
                    "message": message,  # ç›´æ¥ä¿å­˜å®Œæ•´çš„ message å¯¹è±¡
                }

                # è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²å¹¶æ·»åŠ åˆ°listå³ç«¯
                record_json = json.dumps(record, ensure_ascii=False)
                redis_cache.rpush(redis_key, record_json)

                # è®¾ç½®è¿‡æœŸæ—¶é—´
                redis_cache.expire(redis_key, expire_hours * 3600)

                # è·å–å½“å‰æ•°é‡å¹¶é™åˆ¶æ€»æ•°é‡ï¼šä¿ç•™æœ€æ–°çš„100æ¡
                total_count = redis_cache.llen(redis_key)
                if total_count > 100:
                    # ä»å·¦ç«¯åˆ é™¤å¤šä½™çš„æ—§è®°å½•
                    excess_count = total_count - 100
                    for _ in range(excess_count):
                        redis_cache.lpop(redis_key)

                logger.info(
                    f"æˆåŠŸä¿å­˜æ¶ˆæ¯åˆ° Redis (conversation_id: {self.conversation_id}, "
                    f"role: {message.get('role')}, timestamp: {current_timestamp}, total_items: {min(total_count, 100)})"
                )
                return {"result": "success"}  # æˆåŠŸåˆ™ç›´æ¥è¿”å›

            except Exception as e:
                last_exception = e
                retry_count += 1
                logger.warning(
                    f"ä¿å­˜åˆ°Rediså¤±è´¥ (å°è¯• {retry_count}/{max_retries}): {str(e)}"
                )
                if retry_count <= max_retries:
                    time.sleep(retry_delay)

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        logger.error(
            f"ä¿å­˜æ¶ˆæ¯åˆ° Redis å¤±è´¥ (conversation_id: {self.conversation_id}): {str(last_exception)}"
        )
        raise last_exception

    @langfuse_wrapper.dynamic_observe()
    def _load_conversation_history(
        self, expire_hours: int = 12
    ) -> List[Dict[str, Any]]:
        """ä» Redis list ä¸­åŠ è½½å®Œæ•´çš„å¯¹è¯å†å²è®°å½•ï¼Œè¿”å›ä¸è¿è¡Œæ—¶ messages ç»“æ„å®Œå…¨ä¸€è‡´çš„æ•°æ®

        Args:
            expire_hours: è¿‡æœŸæ—¶é—´ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤12å°æ—¶

        Returns:
            List[Dict[str, Any]]: å¯¹è¯å†å²åˆ—è¡¨ï¼Œæ ¼å¼ä¸ messages å®Œå…¨ä¸€è‡´
        """
        try:
            redis_cache = get_redis_connection()
            redis_key = f"conversation:{self.conversation_id}"

            # æ£€æŸ¥keyæ˜¯å¦å­˜åœ¨
            if not redis_cache.exists(redis_key):
                logger.info(f"æœªæ‰¾åˆ°å¯¹è¯å†å² (conversation_id: {self.conversation_id})")
                return []

            # è·å–listé•¿åº¦
            total_count = redis_cache.llen(redis_key)
            if total_count == 0:
                return []

            # è®¡ç®—è¦è·å–çš„è®°å½•æ•°é‡ï¼šè·å–æœ€è¿‘çš„æ‰€æœ‰è®°å½•ï¼ˆåŒ…æ‹¬å·¥å…·è°ƒç”¨ï¼‰
            # ç”±äºå·¥å…·è°ƒç”¨ä¼šå¢åŠ é¢å¤–çš„è®°å½•ï¼Œè¿™é‡Œè·å–æ›´å¤šçš„è®°å½•
            records_to_get = min(
                self.conversation_round * 6, total_count
            )  # æ¯è½®å¯èƒ½åŒ…å«å¤šä¸ªå·¥å…·è°ƒç”¨

            # ä»listå³ç«¯è·å–æœ€æ–°çš„records_to_getæ¡è®°å½•
            start_index = max(0, total_count - records_to_get)
            end_index = total_count - 1

            # è·å–å†å²è®°å½•
            history_data = redis_cache.lrange(redis_key, start_index, end_index)

            # è§£æå¹¶è¿‡æ»¤è¿‡æœŸæ•°æ®ï¼Œç›´æ¥è¿”å›å®Œæ•´çš„ message å¯¹è±¡
            conversation_history: List[Dict[str, Any]] = []
            current_time = time.time()
            expire_timestamp = current_time - (expire_hours * 3600)

            for item_json in history_data:
                try:
                    record = json.loads(item_json)
                    # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                    record_timestamp = record.get("timestamp", current_time)
                    if record_timestamp < expire_timestamp:
                        continue

                    # ç›´æ¥è·å–å®Œæ•´çš„ message å¯¹è±¡
                    message = record.get("message")
                    if message:
                        conversation_history.append(message)
                except (json.JSONDecodeError, Exception) as e:
                    logger.warning(f"è§£æå¯¹è¯å†å²å¤±è´¥: {e}")
                    continue

            if conversation_history:
                logger.info(
                    f"æˆåŠŸåŠ è½½ {len(conversation_history)} æ¡å¯¹è¯å†å²è®°å½• (conversation_id: {self.conversation_id})"
                )
            return conversation_history

        except Exception as e:
            logger.error(f"åŠ è½½å¯¹è¯å†å²å¤±è´¥: {e}")
            return []

    @langfuse_wrapper.dynamic_observe()
    def _build_skills_memories_content(
        self, skills_memories: List[Dict[str, Any]]
    ) -> str:
        """æ„å»ºskillsè®°å¿†å†…å®¹ï¼Œç”¨äºæ¨¡æ¿æ›¿æ¢

        æ ¹æ® skills_manager çš„ä¸¤çº§å¬å›ç»“æ„ï¼Œä»è¿”å›çš„ skill è¯¦æƒ…ä¸­æå–ï¼š
        - skill_description: æŠ€èƒ½æè¿°ï¼ˆç®€æ´è¯´æ˜ï¼‰
        - skill_detail: æŠ€èƒ½è¯¦æƒ…ï¼ˆè¯¦ç»†æ­¥éª¤å’Œæ³¨æ„äº‹é¡¹ï¼‰
        - user_query: åŸå§‹ç”¨æˆ·é—®é¢˜
        - tool_count: å·¥å…·è°ƒç”¨æ•°é‡
        - similarity_score: ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆå¦‚æœæœ‰ï¼‰

        Args:
            skills_memories: ç›¸å…³skillsè®°å¿†åˆ—è¡¨ï¼Œæ¥è‡ªä¸¤çº§å¬å›çš„ç»“æœ

        Returns:
            str: æ ¼å¼åŒ–çš„skillsè®°å¿†å†…å®¹
        """
        if not skills_memories or not self.enable_skills_memory:
            return "æš‚æ— ç›¸å…³ç»éªŒå¯ä¾›å‚è€ƒã€‚"

        # æ„å»ºskillsè®°å¿†æ ‡é¢˜
        skills_content = ""

        for i, memory in enumerate(skills_memories, 1):
            metadata = memory.get("metadata", {})

            # æå–æ ¸å¿ƒå­—æ®µ
            skill_description = metadata.get("skill_description", "")
            skill_detail = metadata.get("skill_detail", "")
            user_query = metadata.get("user_query", "")
            tool_count = metadata.get("tool_count", 0)

            # æå–ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆä¸¤çº§å¬å›çš„ç»¼åˆåˆ†æ•°ï¼‰
            similarity_score = memory.get("similarity_score", 0.0)
            first_stage_score = memory.get("first_stage_score", 0.0)
            second_stage_score = memory.get("second_stage_score", 0.0)

            # è·³è¿‡æ— æ•ˆçš„è®°å¿†
            if not skill_description and not skill_detail:
                continue

            # æ„å»ºæ ‡é¢˜ï¼ˆåŒ…å«ç›¸ä¼¼åº¦ä¿¡æ¯ï¼‰
            skills_content += f"## æŠ€èƒ½ {i}: {skill_description}\n"
            skills_content += f"**ç›¸ä¼¼åº¦**: {similarity_score:.2%}"
            if first_stage_score > 0 or second_stage_score > 0:
                skills_content += f" (æè¿°åŒ¹é…: {first_stage_score:.2%}, è¯¦æƒ…åŒ¹é…: {second_stage_score:.2%})"
            skills_content += "\n\n"

            # æ·»åŠ åŸå§‹é—®é¢˜ï¼ˆæä¾›ä¸Šä¸‹æ–‡ï¼‰
            if user_query:
                skills_content += f"**åŸå§‹é—®é¢˜**: {user_query}\n\n"

            # æ·»åŠ æŠ€èƒ½è¯¦æƒ…ï¼ˆåŒ…å«å…·ä½“æ­¥éª¤å’Œæ³¨æ„äº‹é¡¹ï¼‰
            if skill_detail:
                skills_content += f"**æ‰§è¡Œæ­¥éª¤ä¸è¦ç‚¹**:\n{skill_detail}\n\n"
            elif skill_description:
                # å¦‚æœæ²¡æœ‰è¯¦æƒ…ï¼Œè‡³å°‘æ˜¾ç¤ºæè¿°
                skills_content += f"**æŠ€èƒ½è¯´æ˜**: {skill_description}\n\n"

            # æ·»åŠ å·¥å…·æ•°é‡ä¿¡æ¯
            if tool_count > 0:
                skills_content += f"**å·¥å…·ä½¿ç”¨**: æ¶‰åŠ {tool_count} ä¸ªå·¥å…·è°ƒç”¨\n\n"

            skills_content += "---\n\n"

        # æ·»åŠ ä½¿ç”¨æŒ‡å¯¼
        skills_content += (
            "**ğŸ’¡ ä½¿ç”¨å»ºè®®**:\n"
            "1. å‚è€ƒä¸Šè¿°ç»éªŒä¸­çš„è§£å†³æ€è·¯å’Œæ­¥éª¤\n"
            "2. æ ¹æ®å½“å‰é—®é¢˜çš„å…·ä½“æƒ…å†µçµæ´»è°ƒæ•´å·¥å…·è°ƒç”¨é“¾è·¯\n"
            "3. æ³¨æ„ç»éªŒä¸­æåˆ°çš„å…³é”®å‚æ•°å’Œæ³¨æ„äº‹é¡¹\n"
            "4. ä¼˜å…ˆè€ƒè™‘ç›¸ä¼¼åº¦è¾ƒé«˜çš„ç»éªŒ\n"
        )

        return skills_content

    @langfuse_wrapper.dynamic_observe()
    async def _retrieve_skills_memories(
        self, chat_id: str, user_query: str
    ) -> List[Dict[str, Any]]:
        """æ£€ç´¢ä¸ç”¨æˆ·æŸ¥è¯¢ç›¸å…³çš„skillsè®°å¿†

        Args:
            chat_id: èŠå¤©ä¼šè¯ID
            user_query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬

        Returns:
            List[Dict]: ç›¸å…³skillsè®°å¿†åˆ—è¡¨
        """
        try:
            # ä½¿ç”¨ç”¨æˆ·æŸ¥è¯¢ä½œä¸ºæœç´¢æ¡ä»¶æ£€ç´¢ç›¸å…³skillsè®°å¿†
            skills_memories = await self.skills_manager.search_skills(
                query=user_query,
                n_results=3,  # è¿”å›æœ€ç›¸å…³çš„3ä¸ªskillsè®°å¿†
                user_name=None,  # æš‚æ—¶ä½¿ç”¨å…¨å±€è®°å¿†ï¼Œåç»­å¯æ”¯æŒç”¨æˆ·éš”ç¦»
            )

            logger.info(f"[{chat_id}] æ£€ç´¢åˆ° {len(skills_memories)} ä¸ªç›¸å…³skillsè®°å¿†")
            return skills_memories

        except Exception as e:
            logger.error(f"[{chat_id}] æ£€ç´¢skillsè®°å¿†å¤±è´¥: {str(e)}", exc_info=True)
            return []

    def _get_tool_calls_from_messages(
        self, messages: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """ä»æ¶ˆæ¯å†å²ä¸­æå–å·¥å…·è°ƒç”¨é“¾ä¿¡æ¯

        Args:
            messages: æ¶ˆæ¯å†å²åˆ—è¡¨

        Returns:
            List[Dict]: å·¥å…·è°ƒç”¨é“¾ä¿¡æ¯
        """
        tool_chain = []

        for message in messages:
            if message.get("role") == "assistant" and message.get("tool_calls"):
                for tool_call in message["tool_calls"]:
                    function = tool_call.get("function", {})
                    tool_name = function.get("name", "")
                    arguments = function.get("arguments", "{}")

                    # è§£æå‚æ•°
                    try:
                        if isinstance(arguments, str):
                            action_input = json.loads(arguments)
                        else:
                            action_input = arguments
                    except (json.JSONDecodeError, TypeError):
                        action_input = {}

                    tool_chain.append(
                        {"tool_name": tool_name, "action_input": action_input}
                    )

        return tool_chain

    @langfuse_wrapper.dynamic_observe()
    async def _async_process_skills_memory(
        self,
        chat_id: str,
        user_query: str,
        final_result: str,
        tool_calls_history: List[Dict[str, Any]],
        is_success: bool = True,
    ) -> None:
        """å¼‚æ­¥å¤„ç†skillsè®°å¿†ç”Ÿæˆå’Œä¿å­˜

        Args:
            chat_id: èŠå¤©ä¼šè¯ID
            user_query: ç”¨æˆ·åŸå§‹æŸ¥è¯¢
            final_result: æœ€ç»ˆç»“æœ
            tool_calls_history: å·¥å…·è°ƒç”¨å†å²
            is_success: æ˜¯å¦æˆåŠŸè§£å†³
        """
        try:
            # åªåœ¨æˆåŠŸè§£å†³é—®é¢˜ä¸”æœ‰å·¥å…·è°ƒç”¨æ—¶ç”Ÿæˆskillsè®°å¿†
            if not is_success or not tool_calls_history:
                return

            # å¼‚æ­¥è°ƒç”¨skillsè®°å¿†å¤„ç†
            import asyncio

            asyncio.create_task(
                self._process_single_skills_memory(
                    chat_id=chat_id,
                    user_query=user_query,
                    tool_chain=tool_calls_history,
                    final_result=final_result,
                )
            )

            logger.info(f"[{chat_id}] å·²å¯åŠ¨skillsè®°å¿†å¼‚æ­¥å¤„ç†")

        except Exception as e:
            logger.error(f"[{chat_id}] å¯åŠ¨skillsè®°å¿†å¤„ç†å¤±è´¥: {str(e)}", exc_info=True)

    @langfuse_wrapper.dynamic_observe()
    async def _process_single_skills_memory(
        self,
        chat_id: str,
        user_query: str,
        tool_chain: List[Dict[str, Any]],
        final_result: str,
    ) -> None:
        """å¤„ç†å•ä¸ªskillsè®°å¿†çš„ç”Ÿæˆå’Œä¿å­˜

        Args:
            chat_id: èŠå¤©ä¼šè¯ID
            user_query: ç”¨æˆ·åŸå§‹æŸ¥è¯¢
            tool_chain: å·¥å…·è°ƒç”¨é“¾
            final_result: æœ€ç»ˆç»“æœ
            is_success: æ˜¯å¦æˆåŠŸè§£å†³
        """
        try:
            # è°ƒç”¨skillsè®°å¿†ç®¡ç†å™¨å¤„ç†è®°å¿†
            skills_experience = await self.skills_manager.process_and_save_skill(
                user_query=user_query,
                tool_chain=tool_chain,
                final_result=final_result,
                user_name=self.user_name,
                model_name=self.model_name,
            )

            if skills_experience:
                logger.info(
                    f"[{chat_id}] skillsè®°å¿†ç”ŸæˆæˆåŠŸ: {skills_experience[:100]}..."
                )
            else:
                logger.info(f"[{chat_id}] æœªç”Ÿæˆæ–°çš„skillsè®°å¿†")

        except Exception as e:
            logger.error(f"[{chat_id}] å¤„ç†skillsè®°å¿†å¤±è´¥: {str(e)}", exc_info=True)


from src.utils.dynamic_observer import auto_apply_here

auto_apply_here(
    globals(),
    include=None,
    exclude=[],
    only_in_module=True,
    verbose=False,
)
